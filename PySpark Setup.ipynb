{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting powerbiclient\n",
      "  Using cached powerbiclient-1.0.0-py2.py3-none-any.whl (585 kB)\n",
      "Collecting msal>=1.8.0\n",
      "  Using cached msal-1.12.0-py2.py3-none-any.whl (66 kB)\n",
      "Requirement already satisfied: ipywidgets>=7.0.0 in c:\\users\\jadel\\appdata\\local\\programs\\python\\python38\\lib\\site-packages (from powerbiclient) (7.5.1)\n",
      "Collecting requests>=2.25.1\n",
      "  Using cached requests-2.25.1-py2.py3-none-any.whl (61 kB)\n",
      "Collecting jupyter-ui-poll>=0.1.2\n",
      "  Using cached jupyter_ui_poll-0.1.2-py2.py3-none-any.whl (6.6 kB)\n",
      "Requirement already satisfied: nbformat>=4.2.0 in c:\\users\\jadel\\appdata\\local\\programs\\python\\python38\\lib\\site-packages (from ipywidgets>=7.0.0->powerbiclient) (5.0.8)\n",
      "Requirement already satisfied: ipykernel>=4.5.1 in c:\\users\\jadel\\appdata\\local\\programs\\python\\python38\\lib\\site-packages (from ipywidgets>=7.0.0->powerbiclient) (5.3.4)\n",
      "Requirement already satisfied: traitlets>=4.3.1 in c:\\users\\jadel\\appdata\\local\\programs\\python\\python38\\lib\\site-packages (from ipywidgets>=7.0.0->powerbiclient) (5.0.5)\n",
      "Requirement already satisfied: ipython>=4.0.0 in c:\\users\\jadel\\appdata\\local\\programs\\python\\python38\\lib\\site-packages (from ipywidgets>=7.0.0->powerbiclient) (7.19.0)\n",
      "Requirement already satisfied: widgetsnbextension~=3.5.0 in c:\\users\\jadel\\appdata\\local\\programs\\python\\python38\\lib\\site-packages (from ipywidgets>=7.0.0->powerbiclient) (3.5.1)\n",
      "Requirement already satisfied: tornado>=4.2 in c:\\users\\jadel\\appdata\\local\\programs\\python\\python38\\lib\\site-packages (from ipykernel>=4.5.1->ipywidgets>=7.0.0->powerbiclient) (6.1)\n",
      "Requirement already satisfied: jupyter-client in c:\\users\\jadel\\appdata\\local\\programs\\python\\python38\\lib\\site-packages (from ipykernel>=4.5.1->ipywidgets>=7.0.0->powerbiclient) (6.1.7)\n",
      "Requirement already satisfied: pickleshare in c:\\users\\jadel\\appdata\\local\\programs\\python\\python38\\lib\\site-packages (from ipython>=4.0.0->ipywidgets>=7.0.0->powerbiclient) (0.7.5)\n",
      "Requirement already satisfied: prompt-toolkit!=3.0.0,!=3.0.1,<3.1.0,>=2.0.0 in c:\\users\\jadel\\appdata\\local\\programs\\python\\python38\\lib\\site-packages (from ipython>=4.0.0->ipywidgets>=7.0.0->powerbiclient) (3.0.8)\n",
      "Requirement already satisfied: backcall in c:\\users\\jadel\\appdata\\local\\programs\\python\\python38\\lib\\site-packages (from ipython>=4.0.0->ipywidgets>=7.0.0->powerbiclient) (0.2.0)\n",
      "Requirement already satisfied: pygments in c:\\users\\jadel\\appdata\\local\\programs\\python\\python38\\lib\\site-packages (from ipython>=4.0.0->ipywidgets>=7.0.0->powerbiclient) (2.7.3)\n",
      "Requirement already satisfied: jedi>=0.10 in c:\\users\\jadel\\appdata\\local\\programs\\python\\python38\\lib\\site-packages (from ipython>=4.0.0->ipywidgets>=7.0.0->powerbiclient) (0.17.2)\n",
      "Requirement already satisfied: colorama in c:\\users\\jadel\\appdata\\local\\programs\\python\\python38\\lib\\site-packages (from ipython>=4.0.0->ipywidgets>=7.0.0->powerbiclient) (0.4.4)\n",
      "Requirement already satisfied: decorator in c:\\users\\jadel\\appdata\\local\\programs\\python\\python38\\lib\\site-packages (from ipython>=4.0.0->ipywidgets>=7.0.0->powerbiclient) (4.4.2)\n",
      "Requirement already satisfied: setuptools>=18.5 in c:\\users\\jadel\\appdata\\local\\programs\\python\\python38\\lib\\site-packages (from ipython>=4.0.0->ipywidgets>=7.0.0->powerbiclient) (47.1.0)\n",
      "Requirement already satisfied: parso<0.8.0,>=0.7.0 in c:\\users\\jadel\\appdata\\local\\programs\\python\\python38\\lib\\site-packages (from jedi>=0.10->ipython>=4.0.0->ipywidgets>=7.0.0->powerbiclient) (0.7.1)\n",
      "Requirement already satisfied: PyJWT[crypto]<3,>=1.0.0 in c:\\users\\jadel\\appdata\\local\\programs\\python\\python38\\lib\\site-packages (from msal>=1.8.0->powerbiclient) (2.1.0)\n",
      "Collecting cryptography<4,>=0.6\n",
      "  Using cached cryptography-3.4.7-cp36-abi3-win_amd64.whl (1.6 MB)\n",
      "Requirement already satisfied: cffi>=1.12 in c:\\users\\jadel\\appdata\\local\\programs\\python\\python38\\lib\\site-packages (from cryptography<4,>=0.6->msal>=1.8.0->powerbiclient) (1.14.4)\n",
      "Requirement already satisfied: pycparser in c:\\users\\jadel\\appdata\\local\\programs\\python\\python38\\lib\\site-packages (from cffi>=1.12->cryptography<4,>=0.6->msal>=1.8.0->powerbiclient) (2.20)\n",
      "Requirement already satisfied: jsonschema!=2.5.0,>=2.4 in c:\\users\\jadel\\appdata\\local\\programs\\python\\python38\\lib\\site-packages (from nbformat>=4.2.0->ipywidgets>=7.0.0->powerbiclient) (3.2.0)\n",
      "Requirement already satisfied: jupyter-core in c:\\users\\jadel\\appdata\\local\\programs\\python\\python38\\lib\\site-packages (from nbformat>=4.2.0->ipywidgets>=7.0.0->powerbiclient) (4.7.0)\n",
      "Requirement already satisfied: ipython-genutils in c:\\users\\jadel\\appdata\\local\\programs\\python\\python38\\lib\\site-packages (from nbformat>=4.2.0->ipywidgets>=7.0.0->powerbiclient) (0.2.0)\n",
      "Requirement already satisfied: six>=1.11.0 in c:\\users\\jadel\\appdata\\local\\programs\\python\\python38\\lib\\site-packages (from jsonschema!=2.5.0,>=2.4->nbformat>=4.2.0->ipywidgets>=7.0.0->powerbiclient) (1.15.0)\n",
      "Requirement already satisfied: pyrsistent>=0.14.0 in c:\\users\\jadel\\appdata\\local\\programs\\python\\python38\\lib\\site-packages (from jsonschema!=2.5.0,>=2.4->nbformat>=4.2.0->ipywidgets>=7.0.0->powerbiclient) (0.17.3)\n",
      "Requirement already satisfied: attrs>=17.4.0 in c:\\users\\jadel\\appdata\\local\\programs\\python\\python38\\lib\\site-packages (from jsonschema!=2.5.0,>=2.4->nbformat>=4.2.0->ipywidgets>=7.0.0->powerbiclient) (20.3.0)\n",
      "Requirement already satisfied: wcwidth in c:\\users\\jadel\\appdata\\local\\programs\\python\\python38\\lib\\site-packages (from prompt-toolkit!=3.0.0,!=3.0.1,<3.1.0,>=2.0.0->ipython>=4.0.0->ipywidgets>=7.0.0->powerbiclient) (0.2.5)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in c:\\users\\jadel\\appdata\\local\\programs\\python\\python38\\lib\\site-packages (from requests>=2.25.1->powerbiclient) (1.25.10)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\jadel\\appdata\\local\\programs\\python\\python38\\lib\\site-packages (from requests>=2.25.1->powerbiclient) (2020.6.20)\n",
      "Requirement already satisfied: chardet<5,>=3.0.2 in c:\\users\\jadel\\appdata\\local\\programs\\python\\python38\\lib\\site-packages (from requests>=2.25.1->powerbiclient) (3.0.4)\n",
      "Requirement already satisfied: idna<3,>=2.5 in c:\\users\\jadel\\appdata\\local\\programs\\python\\python38\\lib\\site-packages (from requests>=2.25.1->powerbiclient) (2.10)\n",
      "Requirement already satisfied: notebook>=4.4.1 in c:\\users\\jadel\\appdata\\local\\programs\\python\\python38\\lib\\site-packages (from widgetsnbextension~=3.5.0->ipywidgets>=7.0.0->powerbiclient) (6.1.5)\n",
      "Requirement already satisfied: jinja2 in c:\\users\\jadel\\appdata\\local\\programs\\python\\python38\\lib\\site-packages (from notebook>=4.4.1->widgetsnbextension~=3.5.0->ipywidgets>=7.0.0->powerbiclient) (3.0.1)\n",
      "Requirement already satisfied: Send2Trash in c:\\users\\jadel\\appdata\\local\\programs\\python\\python38\\lib\\site-packages (from notebook>=4.4.1->widgetsnbextension~=3.5.0->ipywidgets>=7.0.0->powerbiclient) (1.5.0)\n",
      "Requirement already satisfied: prometheus-client in c:\\users\\jadel\\appdata\\local\\programs\\python\\python38\\lib\\site-packages (from notebook>=4.4.1->widgetsnbextension~=3.5.0->ipywidgets>=7.0.0->powerbiclient) (0.9.0)\n",
      "Requirement already satisfied: terminado>=0.8.3 in c:\\users\\jadel\\appdata\\local\\programs\\python\\python38\\lib\\site-packages (from notebook>=4.4.1->widgetsnbextension~=3.5.0->ipywidgets>=7.0.0->powerbiclient) (0.9.1)\n",
      "Requirement already satisfied: argon2-cffi in c:\\users\\jadel\\appdata\\local\\programs\\python\\python38\\lib\\site-packages (from notebook>=4.4.1->widgetsnbextension~=3.5.0->ipywidgets>=7.0.0->powerbiclient) (20.1.0)\n",
      "Requirement already satisfied: pyzmq>=17 in c:\\users\\jadel\\appdata\\local\\programs\\python\\python38\\lib\\site-packages (from notebook>=4.4.1->widgetsnbextension~=3.5.0->ipywidgets>=7.0.0->powerbiclient) (20.0.0)\n",
      "Requirement already satisfied: nbconvert in c:\\users\\jadel\\appdata\\local\\programs\\python\\python38\\lib\\site-packages (from notebook>=4.4.1->widgetsnbextension~=3.5.0->ipywidgets>=7.0.0->powerbiclient) (6.0.7)\n",
      "Requirement already satisfied: python-dateutil>=2.1 in c:\\users\\jadel\\appdata\\local\\programs\\python\\python38\\lib\\site-packages (from jupyter-client->ipykernel>=4.5.1->ipywidgets>=7.0.0->powerbiclient) (2.8.1)\n",
      "Requirement already satisfied: pywin32>=1.0 in c:\\users\\jadel\\appdata\\local\\programs\\python\\python38\\lib\\site-packages (from jupyter-core->nbformat>=4.2.0->ipywidgets>=7.0.0->powerbiclient) (300)\n",
      "Requirement already satisfied: pywinpty>=0.5 in c:\\users\\jadel\\appdata\\local\\programs\\python\\python38\\lib\\site-packages (from terminado>=0.8.3->notebook>=4.4.1->widgetsnbextension~=3.5.0->ipywidgets>=7.0.0->powerbiclient) (0.5.7)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in c:\\users\\jadel\\appdata\\local\\programs\\python\\python38\\lib\\site-packages (from jinja2->notebook>=4.4.1->widgetsnbextension~=3.5.0->ipywidgets>=7.0.0->powerbiclient) (2.0.1)\n",
      "Requirement already satisfied: mistune<2,>=0.8.1 in c:\\users\\jadel\\appdata\\local\\programs\\python\\python38\\lib\\site-packages (from nbconvert->notebook>=4.4.1->widgetsnbextension~=3.5.0->ipywidgets>=7.0.0->powerbiclient) (0.8.4)\n",
      "Requirement already satisfied: bleach in c:\\users\\jadel\\appdata\\local\\programs\\python\\python38\\lib\\site-packages (from nbconvert->notebook>=4.4.1->widgetsnbextension~=3.5.0->ipywidgets>=7.0.0->powerbiclient) (3.2.1)Note: you may need to restart the kernel to use updated packages.\n",
      "Requirement already satisfied: nbclient<0.6.0,>=0.5.0 in c:\\users\\jadel\\appdata\\local\\programs\\python\\python38\\lib\\site-packages (from nbconvert->notebook>=4.4.1->widgetsnbextension~=3.5.0->ipywidgets>=7.0.0->powerbiclient) (0.5.1)\n",
      "Requirement already satisfied: jupyterlab-pygments in c:\\users\\jadel\\appdata\\local\\programs\\python\\python38\\lib\\site-packages (from nbconvert->notebook>=4.4.1->widgetsnbextension~=3.5.0->ipywidgets>=7.0.0->powerbiclient) (0.1.2)\n",
      "Requirement already satisfied: defusedxml in c:\\users\\jadel\\appdata\\local\\programs\\python\\python38\\lib\\site-packages (from nbconvert->notebook>=4.4.1->widgetsnbextension~=3.5.0->ipywidgets>=7.0.0->powerbiclient) (0.6.0)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Error parsing requirements for requests: [Errno 2] No such file or directory: 'c:\\\\users\\\\jadel\\\\appdata\\\\local\\\\programs\\\\python\\\\python38\\\\lib\\\\site-packages\\\\requests-2.24.0.dist-info\\\\METADATA'\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Requirement already satisfied: entrypoints>=0.2.2 in c:\\users\\jadel\\appdata\\local\\programs\\python\\python38\\lib\\site-packages (from nbconvert->notebook>=4.4.1->widgetsnbextension~=3.5.0->ipywidgets>=7.0.0->powerbiclient) (0.3)\n",
      "Requirement already satisfied: testpath in c:\\users\\jadel\\appdata\\local\\programs\\python\\python38\\lib\\site-packages (from nbconvert->notebook>=4.4.1->widgetsnbextension~=3.5.0->ipywidgets>=7.0.0->powerbiclient) (0.4.4)\n",
      "Requirement already satisfied: pandocfilters>=1.4.1 in c:\\users\\jadel\\appdata\\local\\programs\\python\\python38\\lib\\site-packages (from nbconvert->notebook>=4.4.1->widgetsnbextension~=3.5.0->ipywidgets>=7.0.0->powerbiclient) (1.4.3)\n",
      "Requirement already satisfied: async-generator in c:\\users\\jadel\\appdata\\local\\programs\\python\\python38\\lib\\site-packages (from nbclient<0.6.0,>=0.5.0->nbconvert->notebook>=4.4.1->widgetsnbextension~=3.5.0->ipywidgets>=7.0.0->powerbiclient) (1.10)\n",
      "Requirement already satisfied: nest-asyncio in c:\\users\\jadel\\appdata\\local\\programs\\python\\python38\\lib\\site-packages (from nbclient<0.6.0,>=0.5.0->nbconvert->notebook>=4.4.1->widgetsnbextension~=3.5.0->ipywidgets>=7.0.0->powerbiclient) (1.4.3)\n",
      "Requirement already satisfied: webencodings in c:\\users\\jadel\\appdata\\local\\programs\\python\\python38\\lib\\site-packages (from bleach->nbconvert->notebook>=4.4.1->widgetsnbextension~=3.5.0->ipywidgets>=7.0.0->powerbiclient) (0.5.1)\n",
      "Requirement already satisfied: packaging in c:\\users\\jadel\\appdata\\local\\programs\\python\\python38\\lib\\site-packages (from bleach->nbconvert->notebook>=4.4.1->widgetsnbextension~=3.5.0->ipywidgets>=7.0.0->powerbiclient) (20.7)\n",
      "Requirement already satisfied: pyparsing>=2.0.2 in c:\\users\\jadel\\appdata\\local\\programs\\python\\python38\\lib\\site-packages (from packaging->bleach->nbconvert->notebook>=4.4.1->widgetsnbextension~=3.5.0->ipywidgets>=7.0.0->powerbiclient) (2.4.7)\n",
      "Installing collected packages: cryptography, requests, msal, jupyter-ui-poll, powerbiclient\n",
      "  Attempting uninstall: requests\n",
      "    Found existing installation: requests 2.24.0\n",
      "    Can't uninstall 'requests'. No files were found to uninstall.\n",
      "Successfully installed cryptography-3.4.7 jupyter-ui-poll-0.1.2 msal-1.12.0 powerbiclient-1.0.0 requests-2.25.1\n"
     ]
    }
   ],
   "source": [
    "pip install powerbiclient"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#https://github.com/microsoft/powerbi-jupyter/blob/main/demo/demo.ipynb\n",
    "from powerbiclient import Report, models\n",
    "from io import StringIO\n",
    "from ipywidgets import interact\n",
    "import requests\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+\n",
      "|hello|\n",
      "+-----+\n",
      "|spark|\n",
      "+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#Open Spark:?\n",
    "import pyspark\n",
    "from pyspark.sql import SparkSession\n",
    "spark=SparkSession.builder.getOrCreate()\n",
    "df=spark.sql(\"select 'spark'as hello\")\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "== Physical Plan ==\n",
      "*(1) Project [spark AS hello#0]\n",
      "+- *(1) Scan OneRowRelation[]\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#Testing Spark Query Plan\n",
    "spark.sql(\"EXPLAIN SELECT 'Spark' as hello\").first()\n",
    "df.explain()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+--------+-----+\n",
      "|train_id| station| time|\n",
      "+--------+--------+-----+\n",
      "|     324|San Jose|9:05a|\n",
      "|     217|San Jose|6:59a|\n",
      "+--------+--------+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import pyspark\n",
    "from pyspark.sql import SparkSession\n",
    "spark=SparkSession.builder.getOrCreate()\n",
    "df=spark.read.csv(\"trainsched.txt\",header=True)\n",
    "df.createOrReplaceTempView(\"schedule\")\n",
    "spark.sql(\"SELECT * FROM schedule WHERE station='San Jose'\").show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['col_name', 'data_type', 'comment']\n"
     ]
    }
   ],
   "source": [
    "result=spark.sql(\"DESCRIBE schedule\")\n",
    "print(result.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+-----+-----+\n",
      "|train_id|start|  end|\n",
      "+--------+-----+-----+\n",
      "|     217|6:06a|6:59a|\n",
      "|     324|7:59a|9:05a|\n",
      "+--------+-----+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "query=\"SELECT train_id,  min(time) as start, max(time) as end FROM schedule GROUP BY train_id\"\n",
    "spark.sql(query).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Row(_c0='The Project Gutenberg EBook of The Adventures of Sherlock Holmes')\n",
      "+--------------------------------------------------------------------+\n",
      "|_c0                                                                 |\n",
      "+--------------------------------------------------------------------+\n",
      "|The Project Gutenberg EBook of The Adventures of Sherlock Holmes    |\n",
      "|by Sir Arthur Conan Doyle                                           |\n",
      "|(#15 in our series by Sir Arthur Conan Doyle)                       |\n",
      "|Copyright laws are changing all over the world. Be sure to check the|\n",
      "|copyright laws for your country before downloading or redistributing|\n",
      "|this or any other Project Gutenberg eBook.                          |\n",
      "|This header should be the first thing seen when viewing this Project|\n",
      "|Gutenberg file.  Please do not remove it.  Do not change or edit the|\n",
      "|header without written permission.                                  |\n",
      "|Please read the \"legal small print                                  |\n",
      "+--------------------------------------------------------------------+\n",
      "only showing top 10 rows\n",
      "\n",
      "Row(v='the project gutenberg ebook of the adventures of sherlock holmes')\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['v']"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Some basic loading tricks\n",
    "from pyspark.sql.functions import lower, col\n",
    "dfS=spark.read.csv('sherlock.txt')\n",
    "print(dfS.first())\n",
    "dfS.count()\n",
    "dfS.show(10, truncate=False)\n",
    "dfS.columns\n",
    "df1=dfS.select(lower(col('_c0')).alias('v'))\n",
    "print(df1.first())\n",
    "df1.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------------------------------------------------------+\n",
      "|v                                                                   |\n",
      "+--------------------------------------------------------------------+\n",
      "|the project gutenberg ebook of the adventures of sherlock holmes    |\n",
      "|by sir arthur conan doyle                                           |\n",
      "|(#15 in our series by sir arthur conan doyle)                       |\n",
      "|copyright laws are changing all over the world. be sure to check the|\n",
      "|copyright laws for your country before downloading or redistributing|\n",
      "|this or any other project gutenberg ebook.                          |\n",
      "|this header should be the first thing seen when viewing this project|\n",
      "|gutenberg file.  please do not remove it.  do not change or edit the|\n",
      "|header without written permission.                                  |\n",
      "|please read the \"legal small print                                  |\n",
      "|ebook and project gutenberg at the bottom of this file.  included is|\n",
      "|important information about your specific rights and restrictions in|\n",
      "|how the file may be used.  you can also find out about how to make a|\n",
      "|donation to project gutenberg                                       |\n",
      "|**welcome to the world of free plain vanilla electronic texts**     |\n",
      "|**ebooks readable by both humans and by computers                   |\n",
      "|*****these ebooks were prepared by thousands of volunteers!*****    |\n",
      "|title: the adventures of sherlock holmes                            |\n",
      "|author: sir arthur conan doyle                                      |\n",
      "|release date: march                                                 |\n",
      "+--------------------------------------------------------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    },
    {
     "ename": "Py4JJavaError",
     "evalue": "An error occurred while calling o325.showString.\n: org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 75.0 failed 1 times, most recent failure: Lost task 0.0 in stage 75.0 (TID 87) (DESKTOP-JUIOPEK executor driver): java.util.regex.PatternSyntaxException: Unclosed character class near index 20\r\n[_|.\\?\\!\",'\\[\\]\\*()\\]\r\n                    ^\r\n\tat java.util.regex.Pattern.error(Unknown Source)\r\n\tat java.util.regex.Pattern.clazz(Unknown Source)\r\n\tat java.util.regex.Pattern.sequence(Unknown Source)\r\n\tat java.util.regex.Pattern.expr(Unknown Source)\r\n\tat java.util.regex.Pattern.compile(Unknown Source)\r\n\tat java.util.regex.Pattern.<init>(Unknown Source)\r\n\tat java.util.regex.Pattern.compile(Unknown Source)\r\n\tat java.lang.String.split(Unknown Source)\r\n\tat org.apache.spark.unsafe.types.UTF8String.split(UTF8String.java:1034)\r\n\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.processNext(Unknown Source)\r\n\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\r\n\tat org.apache.spark.sql.execution.WholeStageCodegenExec$$anon$1.hasNext(WholeStageCodegenExec.scala:755)\r\n\tat org.apache.spark.sql.execution.SparkPlan.$anonfun$getByteArrayRdd$1(SparkPlan.scala:345)\r\n\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2(RDD.scala:898)\r\n\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2$adapted(RDD.scala:898)\r\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\r\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:373)\r\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:337)\r\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)\r\n\tat org.apache.spark.scheduler.Task.run(Task.scala:131)\r\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:497)\r\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1439)\r\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:500)\r\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(Unknown Source)\r\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(Unknown Source)\r\n\tat java.lang.Thread.run(Unknown Source)\r\n\nDriver stacktrace:\r\n\tat org.apache.spark.scheduler.DAGScheduler.failJobAndIndependentStages(DAGScheduler.scala:2258)\r\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2(DAGScheduler.scala:2207)\r\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2$adapted(DAGScheduler.scala:2206)\r\n\tat scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)\r\n\tat scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)\r\n\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)\r\n\tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:2206)\r\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1(DAGScheduler.scala:1079)\r\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1$adapted(DAGScheduler.scala:1079)\r\n\tat scala.Option.foreach(Option.scala:407)\r\n\tat org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:1079)\r\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:2445)\r\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2387)\r\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2376)\r\n\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)\r\n\tat org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:868)\r\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2196)\r\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2217)\r\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2236)\r\n\tat org.apache.spark.sql.execution.SparkPlan.executeTake(SparkPlan.scala:472)\r\n\tat org.apache.spark.sql.execution.SparkPlan.executeTake(SparkPlan.scala:425)\r\n\tat org.apache.spark.sql.execution.CollectLimitExec.executeCollect(limit.scala:47)\r\n\tat org.apache.spark.sql.Dataset.collectFromPlan(Dataset.scala:3696)\r\n\tat org.apache.spark.sql.Dataset.$anonfun$head$1(Dataset.scala:2722)\r\n\tat org.apache.spark.sql.Dataset.$anonfun$withAction$1(Dataset.scala:3687)\r\n\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$5(SQLExecution.scala:103)\r\n\tat org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:163)\r\n\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$1(SQLExecution.scala:90)\r\n\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:775)\r\n\tat org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:64)\r\n\tat org.apache.spark.sql.Dataset.withAction(Dataset.scala:3685)\r\n\tat org.apache.spark.sql.Dataset.head(Dataset.scala:2722)\r\n\tat org.apache.spark.sql.Dataset.take(Dataset.scala:2929)\r\n\tat org.apache.spark.sql.Dataset.getRows(Dataset.scala:301)\r\n\tat org.apache.spark.sql.Dataset.showString(Dataset.scala:338)\r\n\tat sun.reflect.GeneratedMethodAccessor111.invoke(Unknown Source)\r\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(Unknown Source)\r\n\tat java.lang.reflect.Method.invoke(Unknown Source)\r\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\r\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\r\n\tat py4j.Gateway.invoke(Gateway.java:282)\r\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\r\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\r\n\tat py4j.GatewayConnection.run(GatewayConnection.java:238)\r\n\tat java.lang.Thread.run(Unknown Source)\r\nCaused by: java.util.regex.PatternSyntaxException: Unclosed character class near index 20\r\n[_|.\\?\\!\",'\\[\\]\\*()\\]\r\n                    ^\r\n\tat java.util.regex.Pattern.error(Unknown Source)\r\n\tat java.util.regex.Pattern.clazz(Unknown Source)\r\n\tat java.util.regex.Pattern.sequence(Unknown Source)\r\n\tat java.util.regex.Pattern.expr(Unknown Source)\r\n\tat java.util.regex.Pattern.compile(Unknown Source)\r\n\tat java.util.regex.Pattern.<init>(Unknown Source)\r\n\tat java.util.regex.Pattern.compile(Unknown Source)\r\n\tat java.lang.String.split(Unknown Source)\r\n\tat org.apache.spark.unsafe.types.UTF8String.split(UTF8String.java:1034)\r\n\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.processNext(Unknown Source)\r\n\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\r\n\tat org.apache.spark.sql.execution.WholeStageCodegenExec$$anon$1.hasNext(WholeStageCodegenExec.scala:755)\r\n\tat org.apache.spark.sql.execution.SparkPlan.$anonfun$getByteArrayRdd$1(SparkPlan.scala:345)\r\n\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2(RDD.scala:898)\r\n\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2$adapted(RDD.scala:898)\r\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\r\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:373)\r\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:337)\r\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)\r\n\tat org.apache.spark.scheduler.Task.run(Task.scala:131)\r\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:497)\r\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1439)\r\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:500)\r\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(Unknown Source)\r\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(Unknown Source)\r\n\t... 1 more\r\n",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mPy4JJavaError\u001b[0m                             Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-42-b67aa915889e>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     13\u001b[0m \u001b[0mpuncturation\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m\"_|.\\?\\!\\\",\\'\\[\\]\\*()\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     14\u001b[0m \u001b[0mdf4\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mdf2\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mselect\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msplit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'v'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'[%s\\]'\u001b[0m\u001b[1;33m%\u001b[0m\u001b[0mpuncturation\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0malias\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'words'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 15\u001b[1;33m \u001b[0mdf4\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshow\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtruncate\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mFalse\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     16\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     17\u001b[0m \u001b[1;31m#Exploding array\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\jadel\\appdata\\local\\programs\\python\\python38\\lib\\site-packages\\pyspark\\sql\\dataframe.py\u001b[0m in \u001b[0;36mshow\u001b[1;34m(self, n, truncate, vertical)\u001b[0m\n\u001b[0;32m    484\u001b[0m             \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_jdf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshowString\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mn\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m20\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mvertical\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    485\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 486\u001b[1;33m             \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_jdf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshowString\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mn\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtruncate\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mvertical\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    487\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    488\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m__repr__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\jadel\\appdata\\local\\programs\\python\\python38\\lib\\site-packages\\py4j\\java_gateway.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *args)\u001b[0m\n\u001b[0;32m   1302\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1303\u001b[0m         \u001b[0manswer\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgateway_client\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msend_command\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcommand\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1304\u001b[1;33m         return_value = get_return_value(\n\u001b[0m\u001b[0;32m   1305\u001b[0m             answer, self.gateway_client, self.target_id, self.name)\n\u001b[0;32m   1306\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\jadel\\appdata\\local\\programs\\python\\python38\\lib\\site-packages\\pyspark\\sql\\utils.py\u001b[0m in \u001b[0;36mdeco\u001b[1;34m(*a, **kw)\u001b[0m\n\u001b[0;32m    109\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mdeco\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0ma\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkw\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    110\u001b[0m         \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 111\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mf\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0ma\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkw\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    112\u001b[0m         \u001b[1;32mexcept\u001b[0m \u001b[0mpy4j\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mprotocol\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mPy4JJavaError\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    113\u001b[0m             \u001b[0mconverted\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mconvert_exception\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0me\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mjava_exception\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\jadel\\appdata\\local\\programs\\python\\python38\\lib\\site-packages\\py4j\\protocol.py\u001b[0m in \u001b[0;36mget_return_value\u001b[1;34m(answer, gateway_client, target_id, name)\u001b[0m\n\u001b[0;32m    324\u001b[0m             \u001b[0mvalue\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mOUTPUT_CONVERTER\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mtype\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0manswer\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m2\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mgateway_client\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    325\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0manswer\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m==\u001b[0m \u001b[0mREFERENCE_TYPE\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 326\u001b[1;33m                 raise Py4JJavaError(\n\u001b[0m\u001b[0;32m    327\u001b[0m                     \u001b[1;34m\"An error occurred while calling {0}{1}{2}.\\n\"\u001b[0m\u001b[1;33m.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    328\u001b[0m                     format(target_id, \".\", name), value)\n",
      "\u001b[1;31mPy4JJavaError\u001b[0m: An error occurred while calling o325.showString.\n: org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 75.0 failed 1 times, most recent failure: Lost task 0.0 in stage 75.0 (TID 87) (DESKTOP-JUIOPEK executor driver): java.util.regex.PatternSyntaxException: Unclosed character class near index 20\r\n[_|.\\?\\!\",'\\[\\]\\*()\\]\r\n                    ^\r\n\tat java.util.regex.Pattern.error(Unknown Source)\r\n\tat java.util.regex.Pattern.clazz(Unknown Source)\r\n\tat java.util.regex.Pattern.sequence(Unknown Source)\r\n\tat java.util.regex.Pattern.expr(Unknown Source)\r\n\tat java.util.regex.Pattern.compile(Unknown Source)\r\n\tat java.util.regex.Pattern.<init>(Unknown Source)\r\n\tat java.util.regex.Pattern.compile(Unknown Source)\r\n\tat java.lang.String.split(Unknown Source)\r\n\tat org.apache.spark.unsafe.types.UTF8String.split(UTF8String.java:1034)\r\n\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.processNext(Unknown Source)\r\n\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\r\n\tat org.apache.spark.sql.execution.WholeStageCodegenExec$$anon$1.hasNext(WholeStageCodegenExec.scala:755)\r\n\tat org.apache.spark.sql.execution.SparkPlan.$anonfun$getByteArrayRdd$1(SparkPlan.scala:345)\r\n\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2(RDD.scala:898)\r\n\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2$adapted(RDD.scala:898)\r\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\r\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:373)\r\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:337)\r\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)\r\n\tat org.apache.spark.scheduler.Task.run(Task.scala:131)\r\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:497)\r\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1439)\r\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:500)\r\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(Unknown Source)\r\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(Unknown Source)\r\n\tat java.lang.Thread.run(Unknown Source)\r\n\nDriver stacktrace:\r\n\tat org.apache.spark.scheduler.DAGScheduler.failJobAndIndependentStages(DAGScheduler.scala:2258)\r\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2(DAGScheduler.scala:2207)\r\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2$adapted(DAGScheduler.scala:2206)\r\n\tat scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)\r\n\tat scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)\r\n\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)\r\n\tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:2206)\r\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1(DAGScheduler.scala:1079)\r\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1$adapted(DAGScheduler.scala:1079)\r\n\tat scala.Option.foreach(Option.scala:407)\r\n\tat org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:1079)\r\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:2445)\r\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2387)\r\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2376)\r\n\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)\r\n\tat org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:868)\r\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2196)\r\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2217)\r\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2236)\r\n\tat org.apache.spark.sql.execution.SparkPlan.executeTake(SparkPlan.scala:472)\r\n\tat org.apache.spark.sql.execution.SparkPlan.executeTake(SparkPlan.scala:425)\r\n\tat org.apache.spark.sql.execution.CollectLimitExec.executeCollect(limit.scala:47)\r\n\tat org.apache.spark.sql.Dataset.collectFromPlan(Dataset.scala:3696)\r\n\tat org.apache.spark.sql.Dataset.$anonfun$head$1(Dataset.scala:2722)\r\n\tat org.apache.spark.sql.Dataset.$anonfun$withAction$1(Dataset.scala:3687)\r\n\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$5(SQLExecution.scala:103)\r\n\tat org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:163)\r\n\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$1(SQLExecution.scala:90)\r\n\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:775)\r\n\tat org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:64)\r\n\tat org.apache.spark.sql.Dataset.withAction(Dataset.scala:3685)\r\n\tat org.apache.spark.sql.Dataset.head(Dataset.scala:2722)\r\n\tat org.apache.spark.sql.Dataset.take(Dataset.scala:2929)\r\n\tat org.apache.spark.sql.Dataset.getRows(Dataset.scala:301)\r\n\tat org.apache.spark.sql.Dataset.showString(Dataset.scala:338)\r\n\tat sun.reflect.GeneratedMethodAccessor111.invoke(Unknown Source)\r\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(Unknown Source)\r\n\tat java.lang.reflect.Method.invoke(Unknown Source)\r\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\r\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\r\n\tat py4j.Gateway.invoke(Gateway.java:282)\r\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\r\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\r\n\tat py4j.GatewayConnection.run(GatewayConnection.java:238)\r\n\tat java.lang.Thread.run(Unknown Source)\r\nCaused by: java.util.regex.PatternSyntaxException: Unclosed character class near index 20\r\n[_|.\\?\\!\",'\\[\\]\\*()\\]\r\n                    ^\r\n\tat java.util.regex.Pattern.error(Unknown Source)\r\n\tat java.util.regex.Pattern.clazz(Unknown Source)\r\n\tat java.util.regex.Pattern.sequence(Unknown Source)\r\n\tat java.util.regex.Pattern.expr(Unknown Source)\r\n\tat java.util.regex.Pattern.compile(Unknown Source)\r\n\tat java.util.regex.Pattern.<init>(Unknown Source)\r\n\tat java.util.regex.Pattern.compile(Unknown Source)\r\n\tat java.lang.String.split(Unknown Source)\r\n\tat org.apache.spark.unsafe.types.UTF8String.split(UTF8String.java:1034)\r\n\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.processNext(Unknown Source)\r\n\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\r\n\tat org.apache.spark.sql.execution.WholeStageCodegenExec$$anon$1.hasNext(WholeStageCodegenExec.scala:755)\r\n\tat org.apache.spark.sql.execution.SparkPlan.$anonfun$getByteArrayRdd$1(SparkPlan.scala:345)\r\n\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2(RDD.scala:898)\r\n\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2$adapted(RDD.scala:898)\r\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\r\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:373)\r\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:337)\r\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)\r\n\tat org.apache.spark.scheduler.Task.run(Task.scala:131)\r\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:497)\r\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1439)\r\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:500)\r\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(Unknown Source)\r\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(Unknown Source)\r\n\t... 1 more\r\n"
     ]
    }
   ],
   "source": [
    "#contiune testing partition as well as cleaning data\n",
    "#Replacing\n",
    "from pyspark.sql.functions import regexp_replace, split\n",
    "df2=df1.select(regexp_replace('v','Mr\\.','Mr').alias('v'))\n",
    "#df2=df2.select(regexp_replace('v','don\\'t','do not'))\n",
    "df2.show(truncate=False)\n",
    "\n",
    "#Tokenzing\n",
    "#df3=df2.select(split('v','[]').alians('words'))\n",
    "#df3.show(truncate=False)\n",
    "\n",
    "#Split characters \n",
    "puncturation=\"_|.\\?\\!\\\",\\'\\[\\]\\*()\"\n",
    "df4=df2.select(split('v', '[%s\\]'%puncturation).alias('words'))\n",
    "df4.show(truncate=False)\n",
    "\n",
    "#Exploding array\n",
    "df5=df4.select(explode('words').alias('word'))\n",
    "df5.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Partition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Caching, Loagging , and the Spark UI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
